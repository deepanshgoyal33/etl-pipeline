Metadata-Version: 2.1
Name: etl_pipeline
Version: 0.1.0
Summary: Add your description here
Requires-Python: <3.13,>=3.9
Description-Content-Type: text/markdown
Requires-Dist: dagster
Requires-Dist: dagster-cloud
Provides-Extra: dev
Requires-Dist: dagster-webserver; extra == "dev"
Requires-Dist: pytest; extra == "dev"

# Real-time Stock Data ETL Pipeline with Dagster

This project implements an ETL (Extract, Transform, Load) pipeline for processing real-time stock data from Kafka and loading it into a PostgreSQL database. It leverages Dagster for orchestration, asset management, and scheduling.

## Overview

The pipeline performs the following steps:

1. **Extract:** Consumes stock price and volume data from separate Kafka topics.
2. **Transform:**
    * Converts timestamps to datetime objects.
    * Calculates moving averages for price and volume.
    * Joins price and volume data based on symbol and timestamp.
    * Calculates derived metrics (e.g., price-volume ratio).
3. **Load:** Inserts the processed data into PostgreSQL tables, including hourly partitioned tables for stock prices and volume.

## Architecture

The pipeline uses the following components:

* **Kafka:**  Message broker for streaming real-time stock price and volume data.
* **PostgreSQL:** Relational database for storing the processed data.
* **Dagster:** Orchestration tool for managing the ETL process, defining assets, and scheduling jobs.
* **Python:** Programming language used for implementing the ETL logic. Libraries used include `pandas`, `sqlalchemy`, `confluent-kafka`, and `dagster`.

## Implementation Details

### Assets

The pipeline is defined using Dagster assets:

* **`stock_prices_asset`:** Extracts stock price data from Kafka, calculates the price moving average, and loads it into a PostgreSQL table. The table is partitioned hourly based on the timestamp.
* **`stock_volume`:** Extracts stock volume data from Kafka, calculates the volume moving average, and loads it into a PostgreSQL table.
* **`joined_stock_data`:** Joins the `stock_prices_asset` and `stock_volume` data, calculates the price-volume ratio, and loads the combined data into a PostgreSQL table.

### Kafka Client

A custom `KafkaClient` class handles consuming messages from Kafka in batches. It manages offset tracking and commits offsets after successful processing and database insertion. The client also includes error handling and logging.

### PostgreSQL Client

A custom `DBClient` class manages interactions with the PostgreSQL database. It uses SQLAlchemy for database operations and implements connection pooling. The client also includes logic for creating hourly partitions for stock price and volume data. It supports different partition strategies (hourly, daily, monthly) for flexibility. Error handling and retry logic are implemented for robust data loading.

### Dagster Job and Schedule

A Dagster job (`stock_data_job`) defines the dependencies between the assets. It ensures that the `joined_stock_data` asset runs after `stock_prices_asset` and `stock_volume`.

A Dagster schedule (`stock_data_schedule`) runs the `stock_data_job` every 20 minutes. The schedule is configured with an execution timezone of UTC.


## Usage

1. **Install dependencies:** `pip install -r requirements.txt`
2.**Create virtual environment
python -m venv venv

# Activate virtual environment
source venv/bin/activate

# Install requirements
pip install -r requirements.txt
2. **Configure:** Update the `constants.py` file with your Kafka and PostgreSQL connection details.
3. **Run the pipeline:** Use the Dagster UI or CLI to launch the `stock_data_job`.
4. **View results:**  Query the PostgreSQL database to access the processed stock data.

## Future Improvements

* **Data Quality:** Implement data quality checks to ensure data integrity.
* **Alerting:**  Integrate alerting for pipeline failures.
* **Backfilling:** Implement a mechanism for backfilling historical data.
* **Testing:** Add more comprehensive unit and integration tests.
* **Dockerization:** Containerize the pipeline for easier deployment.
* **Scalability:** Explore options for scaling the pipeline to handle larger data volumes.


## Contributing

Contributions are welcome! Please open an issue or submit a pull request. 
